{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YoloV3 implementation and testing using Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step 1: Making the network's layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import torch\n",
    "import torchvision \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import variable\n",
    "import numpy as np\n",
    "from utils import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_cfg(file_path):\n",
    "    #read the layers and store every block as a dictionary\n",
    "    block = {}\n",
    "    blocks = [] \n",
    "    with open(file=file_path, mode='r') as file:\n",
    "        lines = file.read().split('\\n')\n",
    "        lines = [x for x in lines if (len(x)>0)]\n",
    "        lines = (x for x in lines if x[0] != '#')\n",
    "        lines = [x.rstrip().lstrip() for x in lines]\n",
    "    \n",
    "    for line in lines:\n",
    "        if line[0] == \"[\":\n",
    "            if len(block) != 0:\n",
    "                blocks.append(block)\n",
    "                block = {}\n",
    "            block[\"type\"] = line[1:-1].rstrip()\n",
    "        else:\n",
    "            key, value = line.split('=')\n",
    "            block[key.rstrip()] = value.lstrip()\n",
    "    blocks.append(block)\n",
    "        \n",
    "    return blocks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'type': 'convolutional', 'batch_normalize': '1', 'filters': '32', 'size': '3', 'stride': '1', 'pad': '1', 'activation': 'leaky'}\n"
     ]
    }
   ],
   "source": [
    "yolo_blocks = parse_cfg('yolov3.cfg')\n",
    "print(yolo_blocks[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmptyLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EmptyLayer, self).__init__()\n",
    "\n",
    "class DetectionLayer(nn.Module):\n",
    "    def __init__(self, anchors):\n",
    "        super(DetectionLayer, self).__init__()\n",
    "        self.anchors = anchors\n",
    "\n",
    "\n",
    "def create_modules(blocks):\n",
    "    net_info = blocks[0] # get the network info as stored in the first block\n",
    "    module_list = nn.ModuleList()  #network modules\n",
    "    prev_filters = 3       #stores the filters of the previous layer only\n",
    "    output_filters = []    #stores the filters of each layer\n",
    "    \n",
    "    for index, block in enumerate(blocks[1:]): #making a sequential module for each block containing the layers\n",
    "        module = nn.Sequential()\n",
    "        if(block['type'] == 'convolutional'):\n",
    "            try:\n",
    "                batch_normalize = int(block[\"batch_normalize\"])\n",
    "                bias = False\n",
    "            except:\n",
    "                batch_normalize = 0\n",
    "                bias = True                \n",
    "\n",
    "            filters = int(block[\"filters\"])\n",
    "            kernel_size = int(block[\"size\"])\n",
    "            kernel_stride = int(block[\"stride\"])\n",
    "            kernel_padding = int(block[\"pad\"])\n",
    "            activation = block[\"activation\"]\n",
    "            \n",
    "            if kernel_padding:\n",
    "                pad = (kernel_size - 1) // 2\n",
    "            else:\n",
    "                pad = 0\n",
    "            \n",
    "            conv = nn.Conv2d(prev_filters, filters, kernel_size, kernel_stride, pad, bias= bias)\n",
    "            module.add_module(\"conv{0}\".format(index), conv)\n",
    "            \n",
    "            if batch_normalize:\n",
    "                bn = nn.BatchNorm2d(filters)\n",
    "                module.add_module(\"batch_norm{0}\".format(index), bn)\n",
    "                \n",
    "            if activation == \"leaky\":\n",
    "                act = nn.LeakyReLU(0.1, inplace=True)\n",
    "                module.add_module(\"leaky{0}\".format(index), act)\n",
    "                \n",
    "            \n",
    "        elif(block['type'] == 'shortcut'):\n",
    "            shortcut = EmptyLayer()\n",
    "            module.add_module(\"emptylayer{0}\".format(index), shortcut)\n",
    "            \n",
    "            \n",
    "        elif(block['type'] == 'route'):\n",
    "            print(block)\n",
    "            block['layers'] = block['layers'].split(',')\n",
    "            start = int(block['layers'][0])\n",
    "            \n",
    "            try:\n",
    "                end = int(block[\"layers\"][1])\n",
    "            except:\n",
    "                end = 0\n",
    "\n",
    "            # refer to all layers with negative indices\n",
    "            if start > 0:\n",
    "                start -= index\n",
    "            if end > 0:\n",
    "                end -= index\n",
    "                \n",
    "            route = EmptyLayer()\n",
    "            module.add_module(\"route{0}\".format(index), route)\n",
    "            \n",
    "            #getting the total filters out of this routing layer\n",
    "            if end < 0:\n",
    "                filters = output_filters[index + start] + output_filters[index + end]\n",
    "            else:\n",
    "                filters = output_filters[index + start]\n",
    "                \n",
    "        \n",
    "        elif(block['type'] == 'upsample'):\n",
    "            stride = int(block[\"stride\"])\n",
    "            upsample = nn.Upsample(scale_factor=2, mode=\"nearest\")\n",
    "            module.add_module(\"upsample{0}\".format(index), upsample)\n",
    "            \n",
    "            \n",
    "        elif(block['type'] == \"yolo\"):\n",
    "            mask = block[\"mask\"].split(',')\n",
    "            mask = (int(m) for m in mask)\n",
    "            anchors = block[\"anchors\"].split(\",\")\n",
    "            anchors = [int(a) for a in anchors]\n",
    "            anchors = [(anchors[i], anchors[i+1]) for i in range(0, len(anchors),2)]\n",
    "            anchors = [anchors[i] for i in mask]\n",
    "            \n",
    "            detection = DetectionLayer(anchors)\n",
    "            module.add_module(\"detectionlayer{0}\".format(index), detection)\n",
    "            \n",
    "            \n",
    "        module_list.append(module)\n",
    "        prev_filters = filters\n",
    "        output_filters.append(filters)\n",
    "        \n",
    "    return (net_info, module_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'type': 'route', 'layers': '-4'}\n",
      "{'type': 'route', 'layers': '-1, 61'}\n",
      "{'type': 'route', 'layers': '-4'}\n",
      "{'type': 'route', 'layers': '-1, 36'}\n"
     ]
    }
   ],
   "source": [
    "info, lis = create_modules(yolo_blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (detectionlayer106): DetectionLayer()\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lis[106]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Darknet(nn.Module):\n",
    "    def __init__(self, cfg_file):\n",
    "        super(Darknet, self).__init__()\n",
    "        self.blocks = parse_cfg(cfg_file)\n",
    "        self.info, self.module_list = create_modules(self.blocks)\n",
    "        \n",
    "    def load_weights(self, file_name):\n",
    "        file = open(file_name, 'rb')\n",
    "        self.header = torch.from_numpy(np.fromfile(file, np.int32, 5))\n",
    "        self.seen = self.header[3]\n",
    "        weights = np.fromfile(file, np.int32)\n",
    "\n",
    "        ptr = 0\n",
    "        for i in range(len(self.module_list)):\n",
    "            module_type = self.blocks[i+1][\"type\"]\n",
    "            if module_type == \"convolutional\":\n",
    "                model = self.module_list[i]\n",
    "                conv = model[0]\n",
    "                print(conv)\n",
    "                if 'batch_normalization' in self.blocks[i+1]:\n",
    "                    print('batch norm')\n",
    "                    bn = model[1]\n",
    "\n",
    "                    num_bn_biases = bn.bias.numel()   #store number of biases in this batch normalization\n",
    "\n",
    "                    #load the weights and biases of this batch normalization into variables\n",
    "                    bn_biases = torch.from_numpy(weights[ptr:ptr+num_bn_biases])\n",
    "                    ptr += num_bn_biases\n",
    "\n",
    "                    bn_weights = torch.from_numpy(weights[ptr:ptr+num_bn_biases])\n",
    "                    ptr += num_bn_biases\n",
    "\n",
    "                    bn_running_mean = torch.from_numpy(weights[ptr:ptr+num_bn_biases])\n",
    "                    ptr += num_bn_biases\n",
    "\n",
    "                    bn_running_var = torch.from_numpy(weights[ptr:ptr+num_bn_biases])\n",
    "                    ptr += num_bn_biases\n",
    "\n",
    "                    bn_biases = bn_biases.view(*bn.bias.shape)\n",
    "                    bn_weights = bn_weights.view(*bn.weight.shape)\n",
    "                    bn_running_mean = bn_running_mean.view(*bn.running_mean.shape)\n",
    "                    bn_running_var = bn_running_var.view(*bn.running_var.shape)\n",
    "\n",
    "                    bn,bias.data.copy_(bn_biases)\n",
    "                    bn.weights.data.copy_(bn_weights)\n",
    "                    bn.running_mean.data.copy_(bn_running_mean)\n",
    "                    bn.running_var.data.copy_(bn_running_var)\n",
    "\n",
    "                else:\n",
    "                    try:\n",
    "                        num_conv_biases = conv.bias.numel()\n",
    "                        conv_biases = torch.from_numpy(weights[ptr:ptr+num_conv_biases])\n",
    "                        ptr += num_conv_biases\n",
    "\n",
    "                        conv_biases = conv_biases.view(*conv.bias.shape)\n",
    "\n",
    "                        conv.bias.data.copy_(conv_biases)\n",
    "                    except:\n",
    "                        print(\"conv_layer {0} has no biases\".format(i))\n",
    "\n",
    "                num_conv_weights = conv.weight.numel()\n",
    "                conv_weights = torch.from_numpy(weights[ptr:ptr+num_conv_weights])\n",
    "                ptr += num_conv_weights\n",
    "                \n",
    "                conv_weights = conv_weights.view(*conv.weight.shape)\n",
    "                conv.weight.data.copy_(conv_weights)\n",
    "    \n",
    "        \n",
    "    def forward(self, x, CUDA):\n",
    "        modules = self.blocks[1:]\n",
    "        outputs = {}   #stores the output of each layer to perform routing\n",
    "        write = 0\n",
    "        for i, module in enumerate(modules):\n",
    "            if (module[\"type\"] == \"convolutional\" or module[\"type\"] == \"upsample\"):\n",
    "                x = self.module_list[i](x)\n",
    "                \n",
    "                \n",
    "            elif (module[\"type\"] == \"route\"):\n",
    "                layers = module[\"layers\"]\n",
    "                layers = [int(l) for l in layers]\n",
    "                \n",
    "                if (layers[0] > 0):\n",
    "                    layers[0] -= i\n",
    "                    \n",
    "                if len(layers) == 1:\n",
    "                    x = outputs[i + (layers[0])]\n",
    "                    \n",
    "                else:\n",
    "                    if (layers[1]) > 0:\n",
    "                        layers [1] -= i\n",
    "                        \n",
    "                    map1 = outputs[i + layers[0]]\n",
    "                    map2 = outputs[i + layers[1]]\n",
    "                    x = torch.cat((map1, map2), 1 )   #concatenate the feature maps of the referenced layers\n",
    "                \n",
    "            elif(module[\"type\"] == \"shortcut\"):\n",
    "                layer = int(module[\"from\"])\n",
    "                x = outputs[i-1] + outputs[i + layer] #add the feature maps of the referenced layers\n",
    "                \n",
    "            elif(module[\"type\"] == \"yolo\"):\n",
    "                anchors = self.module_list[i][0].anchors\n",
    "                input_dim = int(self.info[\"height\"])\n",
    "                num_classes = int(module[\"classes\"])\n",
    "                \n",
    "                \n",
    "                x = x.data\n",
    "                \n",
    "                x = predict_transform(x, input_dim, anchors, num_classes, CUDA)\n",
    "                if not write:              \n",
    "                    detections = x\n",
    "                    write = 1\n",
    "\n",
    "                else:       \n",
    "                    detections = torch.cat((detections, x), 1)\n",
    "\n",
    "            outputs[i] = x\n",
    "        \n",
    "        return detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "height = int(info['height'])\n",
    "width = int(info['width'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_input(height, width):\n",
    "    img = cv2.imread(\"dog-cycle-car.png\")\n",
    "    img = cv2.resize(img, (height,width))          #Resize to the model input dimensions\n",
    "    img_ =  img[:,:,::-1].transpose((2,0,1))  # convert BGR to RGB\n",
    "    img_ = img_[np.newaxis,:,:,:]/255.0       #Add a channel at 0 (for batch) and Normalise\n",
    "    img_ = torch.from_numpy(img_).float()     \n",
    "    img_ = Variable(img_)                     \n",
    "    return img_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'type': 'route', 'layers': '-4'}\n",
      "{'type': 'route', 'layers': '-1, 61'}\n",
      "{'type': 'route', 'layers': '-4'}\n",
      "{'type': 'route', 'layers': '-1, 36'}\n",
      "tensor([[[1.5061e+01, 1.5927e+01, 1.0008e+02,  ..., 5.5050e-01,\n",
      "          4.9892e-01, 4.5939e-01],\n",
      "         [1.4884e+01, 1.5713e+01, 1.5956e+02,  ..., 4.9898e-01,\n",
      "          5.6183e-01, 4.5446e-01],\n",
      "         [1.4021e+01, 1.5512e+01, 4.3607e+02,  ..., 5.1329e-01,\n",
      "          4.6810e-01, 4.8544e-01],\n",
      "         ...,\n",
      "         [6.0398e+02, 6.0399e+02, 8.6800e+00,  ..., 5.0039e-01,\n",
      "          4.5357e-01, 5.2235e-01],\n",
      "         [6.0381e+02, 6.0351e+02, 1.6545e+01,  ..., 5.2815e-01,\n",
      "          5.1496e-01, 4.7586e-01],\n",
      "         [6.0393e+02, 6.0446e+02, 2.7278e+01,  ..., 4.7974e-01,\n",
      "          5.1203e-01, 5.1178e-01]]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ingramai/anaconda3/envs/cv-nd/lib/python3.6/site-packages/torch/nn/modules/upsampling.py:129: UserWarning: nn.Upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.{} is deprecated. Use nn.functional.interpolate instead.\".format(self.name))\n"
     ]
    }
   ],
   "source": [
    "model = Darknet(\"yolov3.cfg\")\n",
    "inp = get_test_input(height, width)\n",
    "pred = model(inp, CUDA=False)\n",
    "print (pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the pre-trained weights\n",
    "\n",
    "#wget https://pjreddie.com/media/files/yolov3.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18432"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.module_list[1][0].weight.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "conv_layer 0 has no biases\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (3) must match the existing size (864) at non-singleton dimension 3.  Target sizes: [32, 3, 3, 3].  Tensor sizes: [864]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-ebe74cfffbae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'yolov3.weights'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-54-ed72bdbd7d37>\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, file_name)\u001b[0m\n\u001b[1;32m     62\u001b[0m                 \u001b[0mconv_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mptr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mptr\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mnum_conv_weights\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m                 \u001b[0mptr\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnum_conv_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m                 \u001b[0mconv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The expanded size of the tensor (3) must match the existing size (864) at non-singleton dimension 3.  Target sizes: [32, 3, 3, 3].  Tensor sizes: [864]"
     ]
    }
   ],
   "source": [
    "model.load_weights('yolov3.weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "  (batch_norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (leaky1): LeakyReLU(negative_slope=0.1, inplace)\n",
       ")"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.module_list[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (cv-nd)",
   "language": "python",
   "name": "cv-nd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
